# -*- coding: utf-8 -*-
"""data_quality_validation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QTB7mOUicjQTd2cJQI_4gAUuJFkiPr9h

# **Data Quality & Business KPI Monitoring System**

**1. Loading Libraries**
"""

import pandas as pd
import numpy as np

"""**2. Reading and loading dataset**"""

df = pd.read_csv("/content/sample_data/OnlineRetail.csv", encoding="latin1")
df.head()

df.info()

"""**3. Data Quality Checks - Missing Values**"""

missing_customer = df[df["CustomerID"].isna()]
missing_customer_count = missing_customer.shape[0]

missing_customer_count

"""**4. Data Quality Checks - Duplicate Invoices**"""

duplicate_invoices = df[df.duplicated(subset=["InvoiceNo"], keep=False)]
duplicate_invoice_count = duplicate_invoices.shape[0]

duplicate_invoice_count

"""5. Data Quality Checks - Invalid Quantities"""

invalid_quantity = df[df["Quantity"] <= 0]
invalid_quantity_count = invalid_quantity.shape[0]

invalid_quantity_count

"""**6. Data Quality Checks - Invalid Prices**

"""

invalid_price = df[df["UnitPrice"] <= 0]
invalid_price_count = invalid_price.shape[0]

invalid_price_count

"""**7. Data Quality Checks - Future Dates**"""

df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"], errors="coerce")

future_dates = df[df["InvoiceDate"] > pd.Timestamp.today()]
future_date_count = future_dates.shape[0]

future_date_count

"""**8. Data Quality Summary Report**"""

total_records = len(df)
total_records

dq_summary = pd.DataFrame({
    "Issue Type": [
        "Missing Customer ID",
        "Duplicate Invoice Numbers",
        "Invalid Quantity",
        "Invalid Unit Price",
        "Future Invoice Dates"
    ],
    "Failed Records": [
        missing_customer_count,
        duplicate_invoice_count,
        invalid_quantity_count,
        invalid_price_count,
        future_date_count
    ]
})

dq_summary

dq_summary.columns

# Add Failure Percentage
dq_summary["Failure Percentage"] = (
    dq_summary["Failed Records"] / total_records
) * 100

dq_summary

"""**9. Data Quality Score Calculation**"""

# -----------------------------
# DATA QUALITY SCORE CALCULATION
# -----------------------------

# Define weights for each check (must match order)
weights = [0.3, 0.3, 0.2, 0.1, 0.1]

# Calculate weighted impact
dq_summary["Weighted Impact"] = dq_summary["Failure Percentage"] * weights

# Final data quality score
dq_score = 100 - dq_summary["Weighted Impact"].sum()

dq_score

pd.DataFrame({
    "run_date": [pd.Timestamp.today().date()],
    "data_quality_score": [dq_score]
}).to_csv("data_quality_score.csv", index=False)

total_records = df.shape[0]

dq_summary["Failure Percentage"] = round(
    (dq_summary["Failed Records"] / total_records) * 100, 2
)

dq_summary

dq_summary.to_csv("python_data_quality_summary.csv", index=False)

"""**10. Anomaly Detection - Revenue Analysis**"""

## Anomaly Detection â€“ Revenue Spikes & Drops
# Ensure InvoiceDate is datetime
df["InvoiceDate"] = pd.to_datetime(df["InvoiceDate"], errors="coerce")

# Create Revenue column
df["Revenue"] = df["Quantity"] * df["UnitPrice"]

# Aggregate daily revenue
daily_revenue = (
    df.groupby(df["InvoiceDate"].dt.date)["Revenue"]
    .sum()
    .reset_index()
)

# Rename columns for clarity
daily_revenue.columns = ["Date", "Daily Revenue"]

daily_revenue.head()

"""**11. Statistical Threshold Calculation**"""

#stat threshold
mean_revenue = daily_revenue["Daily Revenue"].mean()
std_revenue = daily_revenue["Daily Revenue"].std()

upper_threshold = mean_revenue + 2 * std_revenue
lower_threshold = mean_revenue - 2 * std_revenue

mean_revenue, upper_threshold, lower_threshold

"""**12. Anomaly Flagging and Alert Log**"""

#Flag anaomalies
daily_revenue["Anomaly Type"] = daily_revenue["Daily Revenue"].apply(
    lambda x: "High Spike" if x > upper_threshold
    else ("Sharp Drop" if x < lower_threshold else "Normal")
)

daily_revenue.head()

alert_log = daily_revenue.loc[
    daily_revenue["Anomaly Type"] != "Normal"
].copy()

alert_log["Alert Reason"] = "Revenue deviated significantly from historical average"
alert_log

alert_log.to_csv("revenue_anomaly_alert_log.csv", index=False)